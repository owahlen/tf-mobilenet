{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Inference with TRT FP32 model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "from tensorflow.python.saved_model import tag_constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to TF-TRT FP32...\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 0, 0)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 0, 0)\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: saved_model/mobilenetv2_TFTRT_FP32/assets\n",
      "Done Converting to TF-TRT FP32\n",
      "2020-06-14 20:14:27.539616: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\n",
      "usage: saved_model_cli show [-h] --dir DIR [--all] [--tag_set TAG_SET]\r\n",
      "                            [--signature_def SIGNATURE_DEF_KEY]\r\n",
      "saved_model_cli show: error: the following arguments are required: --dir\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Linked TensorRT version: (7, 0, 0)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 0, 0)\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: saved_model/mobilenetv2_TFTRT_FP32/assets\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('saved_model/mobilenetv2')\n",
    "print('Converting to TF-TRT FP32...')\n",
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n",
    "                                                               max_workspace_size_bytes=8000000000)\n",
    "\n",
    "converter = trt.TrtGraphConverterV2(input_saved_model_dir='saved_model/mobilenetv2',\n",
    "                                    conversion_params=conversion_params)\n",
    "converter.convert()\n",
    "converter.save(output_saved_model_dir='saved_model/mobilenetv2_TFTRT_FP32')\n",
    "print('Done Converting to TF-TRT FP32')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-14 20:15:06.535649: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\n",
      "\r\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n",
      "\r\n",
      "signature_def['__saved_model_init_op']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['__saved_model_init_op'] tensor_info:\r\n",
      "        dtype: DT_INVALID\r\n",
      "        shape: unknown_rank\r\n",
      "        name: NoOp\r\n",
      "  Method name is: \r\n",
      "\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['input'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 160, 160, 3)\r\n",
      "        name: serving_default_input:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['predictions'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: unknown_rank\r\n",
      "        name: PartitionedCall:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n",
      "\r\n",
      "Defined Functions:\r\n",
      "  Function Name: '__call__'\r\n",
      "    Option #1\r\n",
      "      Callable with:\r\n",
      "        Argument #1\r\n",
      "          inputs: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='inputs')\r\n",
      "        Argument #2\r\n",
      "          DType: bool\r\n",
      "          Value: False\r\n",
      "        Argument #3\r\n",
      "          DType: NoneType\r\n",
      "          Value: None\r\n",
      "    Option #2\r\n",
      "      Callable with:\r\n",
      "        Argument #1\r\n",
      "          Input: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='Input')\r\n",
      "        Argument #2\r\n",
      "          DType: bool\r\n",
      "          Value: False\r\n",
      "        Argument #3\r\n",
      "          DType: NoneType\r\n",
      "          Value: None\r\n",
      "    Option #3\r\n",
      "      Callable with:\r\n",
      "        Argument #1\r\n",
      "          Input: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='Input')\r\n",
      "        Argument #2\r\n",
      "          DType: bool\r\n",
      "          Value: True\r\n",
      "        Argument #3\r\n",
      "          DType: NoneType\r\n",
      "          Value: None\r\n",
      "    Option #4\r\n",
      "      Callable with:\r\n",
      "        Argument #1\r\n",
      "          inputs: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='inputs')\r\n",
      "        Argument #2\r\n",
      "          DType: bool\r\n",
      "          Value: True\r\n",
      "        Argument #3\r\n",
      "          DType: NoneType\r\n",
      "          Value: None\r\n",
      "\r\n",
      "  Function Name: '_default_save_signature'\r\n",
      "    Option #1\r\n",
      "      Callable with:\r\n",
      "        Argument #1\r\n",
      "          Input: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='Input')\r\n",
      "\r\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\r\n",
      "    Option #1\r\n",
      "      Callable with:\r\n",
      "        Argument #1\r\n",
      "          Input: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='Input')\r\n",
      "        Argument #2\r\n",
      "          DType: bool\r\n",
      "          Value: False\r\n",
      "        Argument #3\r\n",
      "          DType: NoneType\r\n",
      "          Value: None\r\n",
      "    Option #2\r\n",
      "      Callable with:\r\n",
      "        Argument #1\r\n",
      "          Input: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='Input')\r\n",
      "        Argument #2\r\n",
      "          DType: bool\r\n",
      "          Value: True\r\n",
      "        Argument #3\r\n",
      "          DType: NoneType\r\n",
      "          Value: None\r\n",
      "    Option #3\r\n",
      "      Callable with:\r\n",
      "        Argument #1\r\n",
      "          inputs: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='inputs')\r\n",
      "        Argument #2\r\n",
      "          DType: bool\r\n",
      "          Value: False\r\n",
      "        Argument #3\r\n",
      "          DType: NoneType\r\n",
      "          Value: None\r\n",
      "    Option #4\r\n",
      "      Callable with:\r\n",
      "        Argument #1\r\n",
      "          inputs: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='inputs')\r\n",
      "        Argument #2\r\n",
      "          DType: bool\r\n",
      "          Value: True\r\n",
      "        Argument #3\r\n",
      "          DType: NoneType\r\n",
      "          Value: None\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir saved_model/mobilenetv2_TFTRT_FP32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature keys of optimized model:  ['serving_default']\n",
      "Outputs of serving_default:  {'predictions': TensorSpec(shape=<unknown>, dtype=tf.float32, name='predictions')}\n"
     ]
    }
   ],
   "source": [
    "optimized_model = tf.saved_model.load('saved_model/mobilenetv2_TFTRT_FP32',\n",
    "                                      tags=[tag_constants.SERVING])\n",
    "signature_keys = list(optimized_model.signatures.keys())\n",
    "print('Signature keys of optimized model: ',signature_keys)\n",
    "infer = optimized_model.signatures[trt.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "print('Outputs of serving_default: ', infer.structured_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "tfds.disable_progress_bar()\n",
    "ds, metadata = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split='train',\n",
    "    with_info=True,\n",
    "    as_supervised=True)\n",
    "get_label_name = metadata.features['label'].int2str\n",
    "decode_prediction = lambda x: 1 if x>=0 else 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog[1] image - correct prediction: True\n",
      "dog[1] image - correct prediction: True\n",
      "dog[1] image - correct prediction: True\n"
     ]
    }
   ],
   "source": [
    "for image, label in ds.take(3):\n",
    "    x = tf.cast(image, tf.float32)\n",
    "    x = (x/127.5)-1\n",
    "    x = tf.image.resize(x, (160,160))\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "\n",
    "    preds = infer(x)\n",
    "    prediction = preds['predictions'][0,0] # only process first object at first batch index\n",
    "    decoded_pred = decode_prediction(prediction)\n",
    "    correct_prediction = label == decoded_pred\n",
    "\n",
    "    print('{}[{}] image - correct prediction: {}'.format(get_label_name(label), label, correct_prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}